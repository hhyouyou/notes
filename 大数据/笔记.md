[TOC]

# 大数据



## 数仓 Data Warehouse



### 数仓概念

#### 数仓简介：

数据仓库，存储数据，提供查询/分析等后续业务

#### 输入数据分类： 

业务数据、用户行为数据、爬虫数据



#### 整体流程：

1. 数据采集：
   * 文件数据采集:  flume   (用户行为数据)
   * 数据库数据采集：DataX （业务数据）
2. 数据分析：数据过滤、清洗、聚合、统计
   * ods 贴源层
   * dwd 明细层
   * dws 数据轻度汇总层， 预聚合
   * ads 热数据层，统计最终指标
3. 数据输出：
   * 数据可视化报表
   * 用户画像
   * 推荐系统
   * 机器学习



### 项目需求分析



#### 采集平台

业务数据

用户行为数据

#### 离线需求

用户主题、产品主题、其他业务主题。。。。



#### 实时需求

实时用户、实时产品变化



### 项目框架

#### 技术选型

**考虑因素：**

* 数据量大小：每日数据输入量

* 业务需求：比如只需要离线数据处理，那无需简历实时数仓

* 行业内部经验：常用的框架，方案

* 技术成熟度：用的人多，会的人多

* 开发维护成本：简单易用，贴合业务需求

* 总预算成本：经济/人力成本控制



技术组件



数据采集传输：*Flume*, *Kafka*, *DataX*(全量同步), *Maxwell*(增量同步) , Sqoop，Logstash

数据存储：*MySQL*, *HDFS*, *HBase*, *Redis*, MongoDB(机器学习，爬虫)

数据计算：*Hive*, *Spark*, *Flink*, Storm, Tez

数据查询：*Presto*, Kylin, Impala, Druid, *ClickHouse*, Doris

数据可视化：*Superset*, Echarts, *Sugar*, QuickBI, DataV

任务调度：*DolphinScheduler*, Azkaban, Oozie, Airflow

集群监控：Zabbix, Prometheus(实时监控)

元数据管理：Atlas

权限管理：*Ranger*, Sentry

 



#### 系统数据流程 

...





#### 框架版本选型

**自己部署**

Apache： 组件兼容问题

CDH：一键部署，高版本开始收费

HDP：开源，国内用的少

**云服务**

阿里云EMR

亚马逊云EMR

...



**服务器选型**

物理机

云主机





#### 数据规模



**数据规模估算**

* 每天活跃用户100w, 平均每人100条：100w*100 = 1亿
* 每条日志1k左右，每天1亿条：1亿/1024/1024=100G
* 半年不扩容服务器：100G*180天 = 18T
* 3副本：18T * 3= 54T
* 预留20%~30% Buf = 57T/0.7 = 77T
* 约 8*10T 服务器

数仓分层？数据压缩？





#### 集群资源规划

 生产集群+ 测试集群

组件均衡，资源使用均衡







### 用户行为日志



#### 用户行为日志概述：

日志内容：用户行为信息（操作），环境信息（ip，所处页面）

买点方式：代码买点(前后端)、可视化埋点（配置）、全埋点



#### 用户行为日志内容：

* 页面浏览记录：用户信息/页面信息/时间信息/位置/访问设备/渠道
* 动作记录：用户动作操作
* 曝光记录：曝光对象信息
* 启动记录：启动时间，类型（怎么进来）
* 错误记录：收集错误信息



#### 用户行为日志格式：

约定，通用，易解析的格式 —— json

消息体组成

```json
{
    "统一消息头":{
        "公共信息1":"....",
        "公共信息2":"....",
        "消息类型":"1"
    },
   	"body":[
    	{
			"消息内容":"112233"    
		}
    ]
}
```





### 服务器环境准备

#### 服务器准备

* 操作-系统
* 网络
* 常用工具
* 防火墙关闭
* 用户配置，权限配置
* 磁盘分区，文件夹建立
* hosts映射



#### 云服务器准备

包年包月/按量/抢占临时



#### 编写集群分发脚本

```shell
# 同步目录
rsync -av /opt/module  user@host:/opt/
```

#### SSH免密登录

生成ssh密钥

方法一：交换公钥/私钥

方法二：直接使用同一组密钥

#### JDK

#### 环境变量配置





### 数据采集



#### 数据通道

#### 环境准备



**Hadoop部署**

1. 环境变量配置

2. 配置文件修改

   1. core-site.xml , NameNode地址， hdfs数据目录, 用户配置
   2. hdfs-site.xml :  NameNode的web访问地址 ， 指定副本数，
   3. yarn-site.xml: 指定MR走shuffle, 指定ResourceManager 地址， 环境变量的继承， continer容器大小，nodeManager大小 ， 日志聚集
   4. mapred-side.xml: 程序运行在yarn上， 任务运行记录
   5. workers: 指定hadoop集群有几个节点

3. 集群启动

   1. 格式化NameNode: `hdfs namenode -format`
   2. 起NameNode: `start-dfs.sh`
   3. 启动ResourceManager:  `start-yarn.sh`

4. 多目录配置

   hdfs-site.xml

   dfs.datanode.data.dir: file:///dfs/data1,file:///dfs/data2,file:///dfs/data3,file:///dfs/data4



**数据均衡：**

* 磁盘数据均衡 

  ```shell
  # 生成计划
  hdfs diskbalancer -plan node-01 
  # 执行计划
  hdfs diskbalancer -execute node-01.plan.json
  # 查询任务执行情况
  hdfs diskbalancer -query node-01 
  # 取消任务
  hdfs diskbalancer -cancel node-01 
  ```

* 节点数据均衡

  ```shel
  start-balancer.sh  -threshold 10(比例)
  stop-balancer.sh
  ```



**参数调优**

hdfs-site.xml

```xml
# NameNode工作线程池，处理不同DataNode的并发心跳，以及客户端并发元数据操作 

<property>
	<name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>

20 * log e为底， 集群数量为指数
```

yarn-site.xml

内存使用参数





zookeeper 安装

略

kafka安装

略

flume安装



#### flume 数据采集：

* source:  数据源，用户从数据发生器采集接收数据，source产生数据流，同时会把产生的数据流以Flume的event格式传输到一个或者多个channel。

  * taildir: 实时读取文件数据，支持断点续传

  * avro： flume之间

  * nc

  * exec：不支持断点续传，一般不用

  * spooling: 支持断点，监控文件夹

  * kafka source:  kafka 消费者

* channel：传输通道，短暂的存储容器，将从source处接收到的event格式的数据以队列形式缓存起来，直到它们被sinks消费掉，它在source和sink间起桥梁的作用，channel是一个完整的事务，这一点保证了数据在收发的时候的一致性. 并且它可以和任意数量的source和sink链接。

  * file: 基于磁盘，稳定

  * memory：基于内存，会丢失
  * kafka channel: 比较多
    * 结合source 和sink
    * 只用source
    * 只用sink

* sink: 下沉，用于消费channel传输的数据，将数据源传递到目标源，目标可能是另一个sink，也可能HDFS、HBase，最终将数据存储到集中存储器。
  * hdfs sink
  * kafka sink： kafka 生产者
  * avro



* event：在flume中使用事件作为传输的基本单元。





**flume 配置(从文件到kafka)**

1. 定义组件：

   a1.sources = r1

   a1.channels = c1

2. 配置sources

   a1.sources.r1.type = TAILDIR

   a1.sources.r1.filegroups = f1

   a1.sources.r1.filegroups.f1 = /opt/module/app/log/app.*

   a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json

   a1.sources.r1.interceptors = i1

   a1.sources.r1.interceptors.i1.type =  com.demo.flume.interceptor.ClassName$Builder

3. 配置channels

   a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel

   a1.channels.c1.kafka.bootstrap.servers = node-01:9092,node-02:9092

   a1.channels.c1.kafka.topic = topic_log

   a1.channels.c1.parseAsFlumeEvent = false

4. 配置sinks

   此处不用sink

5. 组装

   a1.sources.r1.channels = c1

启动：

```shell
./flume-gn agent -n al -c conf/ -f job/file_to_kafka.conf -Dflume.root.logger=info,consle
```



**flume 配置(从kafka到hdfs)**



1. 定义组件：

   a1.sources = r1

   a1.channels = c1

   a1.sinks = k1

2. 配置sources

   a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource

   *一批发送多少数据，多少时间一批*

   a1.sources.r1.batchSize=5000

   a1.sources.r1.batchDurationMillis=2000

   a1.sources.r1.kafka.bootstrap.servers = node-01:9092,node-02:9092

   a1.sources.r1.kafka.topic = topic_log

   a1.sources.r1.kafka.consumer.group.id = group_log

   a1.sources.r1.interceptors = i1

   a1.sources.r1.interceptors.i1.type =  com.demo.flume.interceptor.ClassName$Builder

3. 配置channels

   a1.channels.c1.type = file

   *断点续传 check point*

   a1.channels.c1.checkpointDir=/opt/flume/checkpoint/file1

   *flume多目录存储，dataDirs指向不同磁盘目录，会加快写入速度，增大吞吐量*

   a1.channels.c1.dataDirs=/opt/flume/data/file1

   *默认文件大小， 2G*

   a1.channels.c1.maxFileSize=2146435071

   *最大条数，默认100w条*

   a1.channels.c1.capacity = 1000000

   *如果channel满了，等待数据被sink的时间*

   a1.channels.c1.keep-alive=6

4. 配置sinks

   a1.sinks.k1.type = hdfs

   a1.sinks.k1.hdfs.path=/home/warehouse/database/log/%y-%m-%d/%h

   a1.sinks.k1.hdfs.filePrefix = log

   *round ,滚动时间， valuekey= 时分秒 value=时间值*

   a1.sinks.k1.hdfs.round = false

   合并小文件优化：文件滚动，创建超过多少时间/达到128M/多少条数后

   a1.sinks.k1.hdfs.rollInterval = 3600

   a1.sinks.k1.hdfs.rollSize = 134217728

   a1.sinks.k1.hdfs.rollCount = 0

   控制输出的文件类型：

   a1.sinks.k1.hdfs.fileType = CompressedStream

   a1.sinks.k1.hdfs.codec= gzip

5. 组装

   a1.sources.r1.channels = c1

   a1.sinks.k1.channel=c1









## 电商业务简介



略







### Maxwell使用

#### 简介

实时监控Mysql 的数据变更操作，基于Binlog 将变更数据以json格式发送给kafka等流式数据处理平台



插入数据：

```sql
insert into db1.test values(1, 'username');
>>>>
json 数据：
{
	"database":"db1",
	"table":"test",
	"type":"insert",
	"ts":"1679754143",(时间戳)
	"xid":"1234565",(事务id)
	"commit":"true",
	"data":{
		"id":1,
		"name":"username"
	}
}
```

更新数据：

```sql
update db1.test set name='usernameupdate' where id =1;
>>>>
json 数据：
{
	"database":"db1",
	"table":"test",
	"type":"update",
	"ts":"1679754143",(时间戳)
	"xid":"1234565",(事务id)
	"commit":"true",
	"data":{
		"id":1,
		"name":"usernameupdate"
	},
	"old":{
		"name":"username"
	}
}
```

删除数据：

```sql
delete from db1.test where id =1;
>>>>
json 数据：
{
	"database":"db1",
	"table":"test",
	"type":"delete",
	"ts":"1679754143",(时间戳)
	"xid":"1234565",(事务id)
	"commit":"true",
	"data":{
		"id":1,
		"name":"usernameupdate"
	}
}
```



#### 基础配置

配置mysql

* 启用binlog日志，配置row模式，开启binlog的数据库

* 配置用户，权限



修改Maxwell 配置文件:

```shell
vim config.properties

# 可选配置stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis
producer=kafka
kafka.bootstrap.servers=node-01:9092,node-02:9092
kafka_topic=%{database}_%{table}
# mysql 相关配置
host=node-01
user=mysql_root
password=123456
jdbc_options=useSSL=false&serverTimezone=Asia?Shanghai
```



启停

```shell
# 启动
maxwell --config config.properties --daemon
# 停止
kill -9 pid
```



#### 全量同步

使用Maxwell-bootstrap进行全量同步

```shell 
maxwell-bootstrap --database db1 --table tb1 --config /file/conf/config.properties
```

bootstrap 的数据格式：

> * start/complete 仅用于标识开始结束，没有具体数据
> * insert中包含数据
> * ts时间都一致，是bootstrap 开始时间

```json
{
    "database": "db1",
    "table": "tb1",
    "type": "bootstrap-start",
    "ts": 1450557744,
    "data": {}
}
{
    "database": "db1",
    "table": "tb1",
    "type": "bootstrap-insert",
    "ts": 1450557744,
    "data": {
        "txt": "hello"
    }
}
{
    "database": "db1",
    "table": "tb1",
    "type": "bootstrap-insert",
    "ts": 1450557744,
    "data": {
        "txt": "hello!"
    }
}
{
    "database": "db1",
    "table": "tb1",
    "type": "bootstrap-complete",
    "ts": 1450557744,
    "data": {}
}


```









### DataX

#### 概述

DataX是阿里开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库（MySQL、Oracle等）、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。



#### 框架设计

基于不同数据库，调用不用的read/write plugin 进行数据输入输出，核心流程由中间的framework控制。

`source db -> ReadPlugin->framework->WriterPlugin-> target db`



#### 运行流程

Job: 一个Job就是一个数据同步作业，一个Job启动一个进程

Task: 根据不同数据源的切分策略，一个Job会切分成多个Task,Task是DataX作业的最小单元， 每个Task负责一部分数据同步工作。

Task Group: Scheduler调度模块会对Task进行分组，每个Task组称为一个Task Group 每个Task Group 负责一定的并发度运行其所分得的Task,  单个Task Group 的并发度为5.

Reader->Channel->  Writer: 每个Task启动后，都会固定启动 Reader->Channel->Writer的线程来完成同步工作。







#### 使用



一键使用: job.josn==配置文件，

```shell
python bin/datax.py path/to/your/job.json
```



```json
{
    "job": {
        "setting": { //Job配置参数，包括限速配置
            "speed": {
                "channel": 3
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [ //数据源和目的地相关配置
            {
                "reader": { //Reader相关配置
                    "name": "mysqlreader", //Reader名称
                    "parameter": { //Reader参数
                        "username": "root",
                        "password": "root",
                        "column": [
                            "id",
                            "name"
                        ],
                        "where": "id>3",
                        "splitPk": "db_id", // 分片字段
                        "connection": [
                            {
                                "table": [
                                    "table"
                                ],
                                "jdbcUrl": [
                                    "jdbc:mysql://127.0.0.1:3306/database"
                                ]
                            }
                        ]
                    }
                },
                "writer": {
                    "name": "hdfswriter", // Writer名称
                    "parameter": {
                        "defaultFS": "hdfs://xxx:port",
                        "fileType": "orc",
                        "path": "/user/hive/warehouse/writerorc.db/orcfull",
                        "fileName": "xxxx/${dt}",
                        "column": [ // 列信息
                            {
                                "name": "col1",
                                "type": "TINYINT"
                            },
                            {
                                "name": "col2",
                                "type": "SMALLINT"
                            },
                            {
                                "name": "col3",
                                "type": "INT"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\t",
                        "compress": "NONE"
                    }
                }
            }
        ]
    }
}
```



#### DataX传参



```shell
python bin/datax.py -p"-Ddt=yyyy-MM-dd" job/job.json
```



 hdfs 传输文件到mysql

```json
[
    "reader": {
        "name": "hdfsreader",
        "parameter": {
            "defaultFS": "hdfs://hadoop102:8020",
            "path": "/base_province",
            "column": [
                "*"
            ],
            "fileType": "text",
            "compress": "gzip",
            "encoding": "UTF-8",
            "nullFormat": "\\N",
            "fieldDelimiter": "\t",

        }
    },
    "writer": {
            "name": "mysqlwriter",
            "parameter": {
                "username": "root",
                "password": "000000",
                "connection": [
                    {
                        "table": [
                            "test_province"
                        ],
                        "jdbcUrl": "jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&characterEncoding=utf-8"
                    }
                ],
                "column": [
                    "id",
                    "name", 
                    "region_id",
                    "area_code",
                    "iso_code",
                    "iso_3166_2"
                ],
                "writeMode": "replace"
            }
        }
    }
]
```



#### DataX优化

**速度控制**

| 参数                                    | 说明                                |
| --------------------------------------- | ----------------------------------- |
| job.setting.speed.channel               | 总并发数                            |
| job.setting.speed.record                | 总record限速，条数                  |
| job.setting.speed.byte                  | 总byte限速，字节数                  |
| core.transport.job.channel.speed.record | 单个channel的record限速，默认1w条/s |
| core.transport.job.channel.speed.byte   | 单个channel的byte限速，默认1M/s     |

tips:

	1. 如果配置了总的record限速，必须配置单个channel的record限速
	1. 如果配置了总的byte限速，必须配置单个channel的byte限速
	1. 配置了总的限速之后，channel的并发参数就会失效。实际并发速=总byte限速/单个chanel的byte限速



**内存调整**

提升Channel并发数，内存占用会显著增加，因为会在内存中缓存数据。

调整方式：直接修改datax.py脚本 ，或者，加上启动参数

```shell
python /bin/datax.py --jvm="-Xms8G -Xmx8G" /path/to/your/job.json
```

 







